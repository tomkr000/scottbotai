{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "590cd04e-d431-4606-a177-1f3443475d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3011"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = 'FYaTB45.txt'\n",
    "\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfba71d3-f27d-4215-aad1-3ce26e781b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iMessage \\n',\n",
       " 'Fri, Aug 19, 8:02 AM \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " 'Que pasa barrachos?!! \\n',\n",
       " 'Fri, Aug 19, 7:46 PM \\n',\n",
       " 'taylen.peterson+24@gmail.com \\n',\n",
       " '  \\n',\n",
       " '\\n',\n",
       " '1 Reply \\n',\n",
       " 'Am I doing Minnesota right? \\n',\n",
       " 'Litre. Gotta remember, this is Canada Lite\\n',\n",
       " 'Ryan Dalton \\n',\n",
       " '  \\n',\n",
       " 'It makes me nauseous to look at it  \\n',\n",
       " '1 Reply \\n',\n",
       " '1 Reply \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " '  \\n',\n",
       " 'Omg I just had a vision about this! \\n',\n",
       " 'It makes me nauseous to look at it  \\n',\n",
       " '1 Reply \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " '  \\n',\n",
       " 'Good nauseous right?! \\n',\n",
       " 'taylen.peterson+24@gmail.com \\n',\n",
       " '  \\n',\n",
       " 'Good nauseous for sure \\n',\n",
       " 'Sat, Aug 20, 1:22 AM \\n',\n",
       " 'scotty.madsen@gmail.com\\n',\n",
       " '  \\n',\n",
       " '\\n',\n",
       " \"Polish prime minister backs Finland's leader in fight for  right to party   \\n\",\n",
       " 'reuters.com \\n',\n",
       " 'Sat, Aug 20, 9:39 AM \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " '  \\n',\n",
       " 'https://t.co/bUGJPrTGhh \\n',\n",
       " 'Terrible Maps   \\n',\n",
       " 'twitter.com \\n',\n",
       " 'I always considered minnesota armpit of a murica but  ughhh NOLA….    \\n',\n",
       " 'taylen.peterson+24@gmail.com \\n',\n",
       " '  \\n',\n",
       " 'New Orleans is the teflon cloaca of America. Checks out \\n',\n",
       " 'Sat, Aug 20, 9:50 PM \\n',\n",
       " 'Will Dalton\\n',\n",
       " '\\n',\n",
       " 'Anyone know what this sport is? Seems like it might be  something fun to watch while drinking several barley  sodas  \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " 'I couldn’t agree more, but nope. Doesn’t ring a bell.  \\n',\n",
       " 'Sun, Aug 21, 12:36 PM \\n',\n",
       " 'Ryan Dalton \\n',\n",
       " 'Crazy that the laser bison beat the fire horsies by such a  wide margin  \\n',\n",
       " 'Mon, Aug 22, 8:06 PM \\n',\n",
       " 'Will Dalton \\n',\n",
       " '\\n',\n",
       " 'uofmdanceteam on TikTok \\n',\n",
       " 'tiktok.com \\n',\n",
       " 'I think I get why Taylen found himself a California girl\\n',\n",
       " 'Mon, Aug 22, 11:58 PM \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " \"Will you know I can't click the link! Have you ever ou been  comprised?! \\n\",\n",
       " '2 Replies \\n',\n",
       " 'Tue, Aug 23, 8:52 PM \\n',\n",
       " 'Will Dalton \\n',\n",
       " '\\n',\n",
       " 'The Daily Intermission on TikTok \\n',\n",
       " 'tiktok.com \\n',\n",
       " 'Scott, time for you to get on this trend \\n',\n",
       " \"Will you know I can't click the link! Have you ever ou been comprised?! 2 Replies \\n\",\n",
       " 'Will Dalton \\n',\n",
       " 'I have not been comprised. Proof: the People’s Republic  of China is the most powerful country in the history of the  world  \\n',\n",
       " 'taylen.peterson+24@gmail.com \\n',\n",
       " 'I feel like TB12 is one of those guys that was good looking  until 20, was one of the ugliest men alive for 15 years, and  now is sexy again. tbf I blame his barber\\n',\n",
       " 'now is sexy again. tbf I blame his barber \\n',\n",
       " 'Will Dalton \\n',\n",
       " 'Proof that you’re not ugly, you’re just poor \\n',\n",
       " 'reddit.com \\n',\n",
       " 'Vanessa shared this with me  \\n',\n",
       " 'taylen.peterson+24@gmail.com \\n',\n",
       " '10,000% \\n',\n",
       " 'Tue, Aug 23, 11:27 PM \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " \"Ha lol God damn I don't even know Vanessa that well but I  lol’d for some reason  \\n\",\n",
       " 'I forget - is she a Brady fan? \\n',\n",
       " \"Will you know I can't click the link! Have you ever ou been comprised?! 2 Replies \\n\",\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " 'I assume ed and xi both have all of my Pickleball fetish  history. Is there really any other point to be concerned? \\n',\n",
       " 'Wed, Aug 24, 8:16 AM \\n',\n",
       " 'scotty.madsen@gmail.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Ryan Dalton \\n',\n",
       " 'Wed, Aug 24, 10:56 AM \\n',\n",
       " 'Are you in a Pickleball VR device? \\n',\n",
       " 'scotty.madsen@gmail.com \\n',\n",
       " 'There is one :)  \\n',\n",
       " 'I broke my paddle and I’m a complete headcase using my  back up. Got crushed by 71yo lady at tourney Sunday \\n',\n",
       " 'Wed, Aug 24, 12:44 PM \\n',\n",
       " 'Will Dalton\\n',\n",
       " 'Connor on TikTok \\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c112ed8-b652-452a-beb7-8f7f4587a9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scotty.madsen@gmail.com',\n",
       " 'Que pasa barrachos?!!',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'Am I doing Minnesota right?',\n",
       " 'Litre. Gotta remember, this is Canada Lite',\n",
       " 'Ryan Dalton',\n",
       " 'It makes me nauseous to look at it',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'Omg I just had a vision about this!',\n",
       " 'It makes me nauseous to look at it',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'Good nauseous right?!',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'Good nauseous for sure',\n",
       " 'scotty.madsen@gmail.com',\n",
       " \"Polish prime minister backs Finland's leader in fight for  right to party\",\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'I always considered minnesota armpit of a murica but  ughhh NOLA….',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'New Orleans is the teflon cloaca of America. Checks out',\n",
       " 'Will Dalton',\n",
       " 'Anyone know what this sport is? Seems like it might be  something fun to watch while drinking several barley  sodas',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'I couldn’t agree more, but nope. Doesn’t ring a bell.',\n",
       " 'Ryan Dalton',\n",
       " 'Crazy that the laser bison beat the fire horsies by such a  wide margin',\n",
       " 'Will Dalton',\n",
       " 'I think I get why Taylen found himself a California girl',\n",
       " 'scotty.madsen@gmail.com',\n",
       " \"Will you know I can't click the link! Have you ever ou been  comprised?!\",\n",
       " 'Will Dalton',\n",
       " 'Scott, time for you to get on this trend',\n",
       " 'Will Dalton',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'I feel like TB12 is one of those guys that was good looking  until 20, was one of the ugliest men alive for 15 years, and  now is sexy again. tbf I blame his barber',\n",
       " 'now is sexy again. tbf I blame his barber',\n",
       " 'Will Dalton',\n",
       " 'Proof that you’re not ugly, you’re just poor',\n",
       " 'Vanessa shared this with me',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " '10,000%',\n",
       " 'scotty.madsen@gmail.com',\n",
       " \"Ha lol God damn I don't even know Vanessa that well but I  lol’d for some reason\",\n",
       " 'I forget - is she a Brady fan?',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'I assume ed and xi both have all of my Pickleball fetish  history. Is there really any other point to be concerned?',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'Ryan Dalton',\n",
       " 'Are you in a Pickleball VR device?',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'I broke my paddle and I’m a complete headcase using my  back up. Got crushed by 71yo lady at tourney Sunday',\n",
       " 'Will Dalton',\n",
       " 'Will Dalton',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'Will keeps pushing risky behavior!',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'She’d be there for less than an hour. And only that long  because it would take the kitchen a while to cook up her  23 pancakes. Then she’d house them in 10 minutes tops',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'Taylen have you been?',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'I’m a legend in Nimrod',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'Required reading, gents',\n",
       " 'Will Dalton',\n",
       " 'I’m surprised the Chase Daniel wasn’t able to lead the  Lego man hair team to victory over the lily team',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'Wtf Is this on?',\n",
       " 'Will Dalton',\n",
       " 'I’m not sure how to watch this new sport  scotty.madsen@gmail.com',\n",
       " 'I bet people will gather once it’s figured out  Will Dalton',\n",
       " 'I’m hoping I can find a bootleg DVD of the match from  someone on Mission St',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'I want to say bettman will squash that vendor so quick',\n",
       " 'But that’s wrong commissioner',\n",
       " 'scotty.madsen@gmail.com',\n",
       " 'What’s a fun place to eat in soma? Have a quasi date (w/  our hr lady) on Sept 16th. I’ve only eaten at chipotle  and Lil chihuahua past eight years.',\n",
       " 'Will Dalton',\n",
       " 'There’s Soma Streat food with a bunch of food trucks',\n",
       " 'People like Tropisueño if you want to pound margaritas',\n",
       " 'What vibe are you going for? If you want a dive bar,  Tempest has food next door you can bring in',\n",
       " 'scotty.madsen@gmail.com',\n",
       " \"Unclear. She's very much happy hour deal chaser down\",\n",
       " \"Unclear. She's very much happy hour deal chaser down  for anything but also brought up the place in ryans old  oak hood that costs $250 a plate... I think she will want sit  down though.\",\n",
       " 'I love margs and tempest!',\n",
       " 'Will Dalton',\n",
       " 'Yeah, Tempest is my second favorite SF dive bar after  Bender’s. I spent a lot of Marissa’s money there',\n",
       " 'taylen.peterson+24@gmail.com',\n",
       " 'Can vouch for all these spots. Hotel Utah for drinks and  live music. Not sure about food there though',\n",
       " 'Ryan Dalton',\n",
       " 'If you’re willing to go to the wrong side of Market,  Mikkeller is great and impressive for anyone who hasn’t  seen it',\n",
       " 'For chill vibes and secret neighborhood spot points,  Garaje is great. Solid food and solid beers',\n",
       " 'Zero Zero is old school by now but still rad. And Local  Kitchen is really good if you want to go slightly fancier  and also contribute to a long-standing con',\n",
       " 'Tell the owner you know me and he’ll either shit his pants  or comp you',\n",
       " 'Maybe both',\n",
       " 'Or you could tell her you’re paleo and bring your own jars  to Rainbow to fill with raw bulgur wheat and wild  mushrooms. If you do that tho could you pick me up some  Wise bagels',\n",
       " 'I hear the new Cellarmaker pizza place is legit if you think  that Detroit pizza is a thing',\n",
       " 'Ryan Dalton',\n",
       " 'One thing a lot of people under-appreciate about SoMa is',\n",
       " 'One thing a lot of people under-appreciate about SoMa is  you can basically poo anywhere, just in case you need  conversation points',\n",
       " 'You can tell her it’s the original human resource  scotty.madsen@gmail.com']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explicit_delete = ['\\n', '  \\n']\n",
    "                   \n",
    "if_found = ['TikTok', 'tiktok.com', 'twitter.com', ':', '1 Reply \\n', '2 Replies',\n",
    "           'reuters.com', 'Terrible Maps', 'defector.com', 'Click to Load Preview', \n",
    "            'GIPHY', 'reddit.com', '3 Replies', 'iMessage']\n",
    "                   \n",
    "cleaned = []\n",
    "    \n",
    "for l in lines:\n",
    "    \n",
    "    add = True\n",
    "    \n",
    "    if l in explicit_delete:\n",
    "        add = False\n",
    "        \n",
    "    for word in if_found:\n",
    "        if word in l:\n",
    "            add = False\n",
    "            \n",
    "    if add:\n",
    "        l = l.replace(\"\\n\", \"\").strip()\n",
    "        cleaned.append(l)\n",
    "        \n",
    "                   \n",
    "cleaned[:100]                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00672dea-6bc6-4d4a-b680-69f34f315054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2214/2214 [00:00<00:00, 5256.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scotty.madsen@gmail.com</td>\n",
       "      <td>Que pasa barrachos?!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>taylen.peterson+24@gmail.com</td>\n",
       "      <td>Am I doing Minnesota right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>taylen.peterson+24@gmail.com</td>\n",
       "      <td>Litre. Gotta remember, this is Canada Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ryan Dalton</td>\n",
       "      <td>It makes me nauseous to look at it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scotty.madsen@gmail.com</td>\n",
       "      <td>Omg I just had a vision about this!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>scotty.madsen@gmail.com</td>\n",
       "      <td>Sign this girl will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>Will Dalton</td>\n",
       "      <td>She has to burn all her Lulu first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>taylen.peterson+24@gmail.com</td>\n",
       "      <td>Ummmmm did y’all see the Oregon North Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>taylen.peterson+24@gmail.com</td>\n",
       "      <td>I will know Gophers football has turned a corn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>taylen.peterson+24@gmail.com</td>\n",
       "      <td>Pope Emeritus RandBall's Stu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1399 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            person  \\\n",
       "0          scotty.madsen@gmail.com   \n",
       "1     taylen.peterson+24@gmail.com   \n",
       "2     taylen.peterson+24@gmail.com   \n",
       "3                      Ryan Dalton   \n",
       "4          scotty.madsen@gmail.com   \n",
       "...                            ...   \n",
       "1394       scotty.madsen@gmail.com   \n",
       "1395                   Will Dalton   \n",
       "1396  taylen.peterson+24@gmail.com   \n",
       "1397  taylen.peterson+24@gmail.com   \n",
       "1398  taylen.peterson+24@gmail.com   \n",
       "\n",
       "                                                   line  \n",
       "0                                 Que pasa barrachos?!!  \n",
       "1                           Am I doing Minnesota right?  \n",
       "2            Litre. Gotta remember, this is Canada Lite  \n",
       "3                    It makes me nauseous to look at it  \n",
       "4                   Omg I just had a vision about this!  \n",
       "...                                                 ...  \n",
       "1394                                Sign this girl will  \n",
       "1395                 She has to burn all her Lulu first  \n",
       "1396  Ummmmm did y’all see the Oregon North Carolina...  \n",
       "1397  I will know Gophers football has turned a corn...  \n",
       "1398                       Pope Emeritus RandBall's Stu  \n",
       "\n",
       "[1399 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tqdm import tqdm\n",
    "\n",
    "peeps = {'scotty.madsen@gmail.com': 'scott', \n",
    "         'Ryan Dalton': 'rye',\n",
    "         'taylen.peterson+24@gmail.com': 'tater',\n",
    "         'Will Dalton': 'will'\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(columns=['person', 'line'])\n",
    "\n",
    "person = 'scotty.madsen@gmail.com'\n",
    "comment = False\n",
    "\n",
    "for line in tqdm(cleaned):\n",
    "    \n",
    "    comment = True\n",
    "    for peep in peeps.keys():\n",
    "        # print(peep)\n",
    "        if peep in line:\n",
    "            \n",
    "            person = peep\n",
    "            comment = False\n",
    "            \n",
    "    if comment:\n",
    "        df = df.append({'person': person, 'line': line}, ignore_index=True)\n",
    "            \n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84a8c202-8905-4fe2-9c1c-f5257d8ab7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Polish prime minister backs Finland's leader i...</td>\n",
       "      <td>Good nauseous for sure</td>\n",
       "      <td>Good nauseous right?!</td>\n",
       "      <td>It makes me nauseous to look at it</td>\n",
       "      <td>Omg I just had a vision about this!</td>\n",
       "      <td>It makes me nauseous to look at it</td>\n",
       "      <td>Litre. Gotta remember, this is Canada Lite</td>\n",
       "      <td>Am I doing Minnesota right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I always considered minnesota armpit of a muri...</td>\n",
       "      <td>Polish prime minister backs Finland's leader i...</td>\n",
       "      <td>Good nauseous for sure</td>\n",
       "      <td>Good nauseous right?!</td>\n",
       "      <td>It makes me nauseous to look at it</td>\n",
       "      <td>Omg I just had a vision about this!</td>\n",
       "      <td>It makes me nauseous to look at it</td>\n",
       "      <td>Litre. Gotta remember, this is Canada Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I couldn’t agree more, but nope. Doesn’t ring ...</td>\n",
       "      <td>Anyone know what this sport is? Seems like it ...</td>\n",
       "      <td>New Orleans is the teflon cloaca of America. C...</td>\n",
       "      <td>I always considered minnesota armpit of a muri...</td>\n",
       "      <td>Polish prime minister backs Finland's leader i...</td>\n",
       "      <td>Good nauseous for sure</td>\n",
       "      <td>Good nauseous right?!</td>\n",
       "      <td>It makes me nauseous to look at it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will you know I can't click the link! Have you...</td>\n",
       "      <td>I think I get why Taylen found himself a Calif...</td>\n",
       "      <td>Crazy that the laser bison beat the fire horsi...</td>\n",
       "      <td>I couldn’t agree more, but nope. Doesn’t ring ...</td>\n",
       "      <td>Anyone know what this sport is? Seems like it ...</td>\n",
       "      <td>New Orleans is the teflon cloaca of America. C...</td>\n",
       "      <td>I always considered minnesota armpit of a muri...</td>\n",
       "      <td>Polish prime minister backs Finland's leader i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ha lol God damn I don't even know Vanessa that...</td>\n",
       "      <td>10,000%</td>\n",
       "      <td>Vanessa shared this with me</td>\n",
       "      <td>Proof that you’re not ugly, you’re just poor</td>\n",
       "      <td>now is sexy again. tbf I blame his barber</td>\n",
       "      <td>I feel like TB12 is one of those guys that was...</td>\n",
       "      <td>Scott, time for you to get on this trend</td>\n",
       "      <td>Will you know I can't click the link! Have you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Polish prime minister backs Finland's leader i...   \n",
       "1  I always considered minnesota armpit of a muri...   \n",
       "2  I couldn’t agree more, but nope. Doesn’t ring ...   \n",
       "3  Will you know I can't click the link! Have you...   \n",
       "4  Ha lol God damn I don't even know Vanessa that...   \n",
       "\n",
       "                                             context  \\\n",
       "0                             Good nauseous for sure   \n",
       "1  Polish prime minister backs Finland's leader i...   \n",
       "2  Anyone know what this sport is? Seems like it ...   \n",
       "3  I think I get why Taylen found himself a Calif...   \n",
       "4                                            10,000%   \n",
       "\n",
       "                                           context/0  \\\n",
       "0                              Good nauseous right?!   \n",
       "1                             Good nauseous for sure   \n",
       "2  New Orleans is the teflon cloaca of America. C...   \n",
       "3  Crazy that the laser bison beat the fire horsi...   \n",
       "4                        Vanessa shared this with me   \n",
       "\n",
       "                                           context/1  \\\n",
       "0                 It makes me nauseous to look at it   \n",
       "1                              Good nauseous right?!   \n",
       "2  I always considered minnesota armpit of a muri...   \n",
       "3  I couldn’t agree more, but nope. Doesn’t ring ...   \n",
       "4       Proof that you’re not ugly, you’re just poor   \n",
       "\n",
       "                                           context/2  \\\n",
       "0                Omg I just had a vision about this!   \n",
       "1                 It makes me nauseous to look at it   \n",
       "2  Polish prime minister backs Finland's leader i...   \n",
       "3  Anyone know what this sport is? Seems like it ...   \n",
       "4          now is sexy again. tbf I blame his barber   \n",
       "\n",
       "                                           context/3  \\\n",
       "0                 It makes me nauseous to look at it   \n",
       "1                Omg I just had a vision about this!   \n",
       "2                             Good nauseous for sure   \n",
       "3  New Orleans is the teflon cloaca of America. C...   \n",
       "4  I feel like TB12 is one of those guys that was...   \n",
       "\n",
       "                                           context/4  \\\n",
       "0         Litre. Gotta remember, this is Canada Lite   \n",
       "1                 It makes me nauseous to look at it   \n",
       "2                              Good nauseous right?!   \n",
       "3  I always considered minnesota armpit of a muri...   \n",
       "4           Scott, time for you to get on this trend   \n",
       "\n",
       "                                           context/5  \n",
       "0                        Am I doing Minnesota right?  \n",
       "1         Litre. Gotta remember, this is Canada Lite  \n",
       "2                 It makes me nauseous to look at it  \n",
       "3  Polish prime minister backs Finland's leader i...  \n",
       "4  Will you know I can't click the link! Have you...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexted = []\n",
    "\n",
    "n = 7\n",
    "\n",
    "for i in range(n, len(df['line'])):\n",
    "    if df['person'].iloc[i] == 'scotty.madsen@gmail.com':\n",
    "        row = []\n",
    "        prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
    "        for j in range(i, prev, -1):\n",
    "            row.append(df['line'][j])\n",
    "        contexted.append(row) \n",
    "        \n",
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "\n",
    "df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8735fb66-3ce0-4ebc-8e7c-84055a06a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output-small'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f8a6367-6319-4f29-a88f-cae87bea74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/miniforge3/envs/sd2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a10749d9-f3e8-4e57-ae98-0d7352782412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6516ba59-40cc-4242-8195-0d15755a64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02b412fa-729b-46e6-a6f8-4585dbf329ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b84c2e01-0957-4064-a0e9-db78f7da7e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f388711-3456-4bec-8b8d-b15872a04104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>This means we should bet on Mexico eh?!</td>\n",
       "      <td>I think Tim called it. 4-0 with four extra poi...</td>\n",
       "      <td>Willy where did we land</td>\n",
       "      <td>God damn 36.5 odds</td>\n",
       "      <td>So far Georgia is the only one on the guest li...</td>\n",
       "      <td>Source blown!!!!!!</td>\n",
       "      <td>Ive seen gopher traps hanging in taylens garage!</td>\n",
       "      <td>right thing to do for the season opener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>The gap between your current life and the life...</td>\n",
       "      <td>Feel pretty necessary to endure these games</td>\n",
       "      <td>I’ll go if they throw in free barley soda</td>\n",
       "      <td>Stewart Mandel</td>\n",
       "      <td>Stanford is offering a \"free trial\" - - aka, f...</td>\n",
       "      <td>FYI</td>\n",
       "      <td>Pretty sure your tv got numba one spot on most...</td>\n",
       "      <td>weekly crash course in cursing!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Eli &amp; Pete Davidson Hang at Pete's Apartment &amp;...</td>\n",
       "      <td>Instagram Account | The Eli Manning Show</td>\n",
       "      <td>Eli &amp; Pete Davidson Hang at Pete's Apartment &amp;...</td>\n",
       "      <td>4 Replies</td>\n",
       "      <td>(Never noticed how we did it in the past)</td>\n",
       "      <td>Wait commish - shouldn’t division winner get t...</td>\n",
       "      <td>club members left</td>\n",
       "      <td>the back of it and showed bunch of old random ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>I’ll need at least three sessions w/ Tim befor...</td>\n",
       "      <td>I’ll need at least three sessions w/ Tim befor...</td>\n",
       "      <td>Is this the final final final final straw for ...</td>\n",
       "      <td>Speaking of, is Final Final still in business?</td>\n",
       "      <td>on</td>\n",
       "      <td>Needs one final “_FINAL” at the end but otherw...</td>\n",
       "      <td>finalfinalfinal2rev3reallythistime2.scottbot</td>\n",
       "      <td>Is this the final final final final straw for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>*chicken finger reality</td>\n",
       "      <td>into my chicken finger stereotype</td>\n",
       "      <td>Detroit lasagna pizza sounds great but I suspe...</td>\n",
       "      <td>Been awhile since I’ve seen a human taking a poo</td>\n",
       "      <td>One thing a lot of people under-appreciate abo...</td>\n",
       "      <td>Detroit lasagna pizza sounds great but I suspe...</td>\n",
       "      <td>goo.gl</td>\n",
       "      <td>Francisco, CA 94107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              response  \\\n",
       "39             This means we should bet on Mexico eh?!   \n",
       "345  The gap between your current life and the life...   \n",
       "431  Eli & Pete Davidson Hang at Pete's Apartment &...   \n",
       "67   I’ll need at least three sessions w/ Tim befor...   \n",
       "26                             *chicken finger reality   \n",
       "\n",
       "                                               context  \\\n",
       "39   I think Tim called it. 4-0 with four extra poi...   \n",
       "345        Feel pretty necessary to endure these games   \n",
       "431           Instagram Account | The Eli Manning Show   \n",
       "67   I’ll need at least three sessions w/ Tim befor...   \n",
       "26                   into my chicken finger stereotype   \n",
       "\n",
       "                                             context/0  \\\n",
       "39                             Willy where did we land   \n",
       "345          I’ll go if they throw in free barley soda   \n",
       "431  Eli & Pete Davidson Hang at Pete's Apartment &...   \n",
       "67   Is this the final final final final straw for ...   \n",
       "26   Detroit lasagna pizza sounds great but I suspe...   \n",
       "\n",
       "                                            context/1  \\\n",
       "39                                 God damn 36.5 odds   \n",
       "345                                    Stewart Mandel   \n",
       "431                                         4 Replies   \n",
       "67     Speaking of, is Final Final still in business?   \n",
       "26   Been awhile since I’ve seen a human taking a poo   \n",
       "\n",
       "                                             context/2  \\\n",
       "39   So far Georgia is the only one on the guest li...   \n",
       "345  Stanford is offering a \"free trial\" - - aka, f...   \n",
       "431          (Never noticed how we did it in the past)   \n",
       "67                                                  on   \n",
       "26   One thing a lot of people under-appreciate abo...   \n",
       "\n",
       "                                             context/3  \\\n",
       "39                                  Source blown!!!!!!   \n",
       "345                                                FYI   \n",
       "431  Wait commish - shouldn’t division winner get t...   \n",
       "67   Needs one final “_FINAL” at the end but otherw...   \n",
       "26   Detroit lasagna pizza sounds great but I suspe...   \n",
       "\n",
       "                                             context/4  \\\n",
       "39    Ive seen gopher traps hanging in taylens garage!   \n",
       "345  Pretty sure your tv got numba one spot on most...   \n",
       "431                                  club members left   \n",
       "67        finalfinalfinal2rev3reallythistime2.scottbot   \n",
       "26                                              goo.gl   \n",
       "\n",
       "                                             context/5  \n",
       "39             right thing to do for the season opener  \n",
       "345                    weekly crash course in cursing!  \n",
       "431  the back of it and showed bunch of old random ...  \n",
       "67   Is this the final final final final straw for ...  \n",
       "26                                 Francisco, CA 94107  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size = 0.1)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b652c67b-598c-4394-8a58-ed67b950dd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/30/2022 10:50:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 0, distributed training: False, 16-bits training: False\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'set_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(df_trn, df_val)\u001b[0m\n\u001b[1;32m     37\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess rank: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, device: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, n_gpu: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, distributed training: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, 16-bits training: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m     args\u001b[38;5;241m.\u001b[39mlocal_rank,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     args\u001b[38;5;241m.\u001b[39mfp16,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Set seed\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mset_seed\u001b[49m(args)\n\u001b[1;32m     49\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(args\u001b[38;5;241m.\u001b[39mconfig_name, cache_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mcache_dir)\n\u001b[1;32m     50\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(args\u001b[38;5;241m.\u001b[39mtokenizer_name, cache_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mcache_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_seed' is not defined"
     ]
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fbe3f-81bf-4959-b952-f173f57b5050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
